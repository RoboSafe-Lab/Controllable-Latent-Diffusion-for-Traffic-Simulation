[34m[1mwandb[0m: logging graph, to disable use `wandb.watch(log_graph=False)`
Monitoring metrics val/loss under alias val_loss
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
data_cfg.trajdata_predict_types ['vehicle']
{'cache_location': '~/my_custom_cache_location', 'desired_data': ['nusc_trainval-train', 'nusc_trainval-train_val'], 'desired_dt': 0.1, 'future_sec': (5.2, 5.2), 'history_sec': (3.0, 3.0), 'data_dirs': <configs.custom_config.ConfigBase object at 0x7dec8563ddf0>, 'only_types': [<AgentType.VEHICLE: 1>], 'only_predict': [<AgentType.VEHICLE: 1>], 'agent_interaction_distances': defaultdict(<function PassUnifiedDataModule.setup.<locals>.<lambda> at 0x7dec788d9e50>, {}), 'incl_raster_map': True, 'raster_map_params': {'px_per_m': 2, 'map_size_px': 224, 'return_rgb': False, 'offset_frac_xy': [-0.5, 0.0], 'no_map_fill_value': -1.0}, 'incl_vector_map': True, 'centric': 'agent', 'scene_description_contains': None, 'standardize_data': True, 'verbose': True, 'max_agent_num': None, 'num_workers': 24, 'rebuild_cache': False, 'rebuild_maps': False, 'extras': {'closest_lane_point': <function get_closest_lane_point_wrapper.<locals>.get_closest_lane_point at 0x7dec788d9f70>, 'full_fut_traj': <function get_full_fut_traj at 0x7dec30d53310>, 'full_fut_valid': <function get_full_fut_valid at 0x7dec30d533a0>}}
/home/visier/ctg_project/trajdata/src/trajdata/dataset_specific/nusc/nusc_dataset.py:62: UserWarning: Beware, nusc_test has no annotations!
  warnings.warn("Beware, nusc_test has no annotations!")
Loading data for matched scene tags: ['boston-train-nusc_trainval', 'singapore-train-nusc_trainval', 'boston-train_val-nusc_trainval', 'singapore-train_val-nusc_trainval']
Calculating Agent Data (Serially): 100%|â–ˆâ–ˆ| 700/700 [00:00<00:00, 118526.21it/s]
700 scenes in the scene index.
Creating Agent Data Index (24 CPUs): 100%|â–ˆ| 700/700 [00:00<00:00, 19237.66it/s]
Structuring Agent Data Index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 700/700 [00:00<00:00, 70909.62it/s]
Loading data for matched scene tags: ['boston-val-nusc_trainval', 'singapore-val-nusc_trainval']
Calculating Agent Data (Serially): 100%|â–ˆâ–ˆ| 150/150 [00:00<00:00, 113380.00it/s]
150 scenes in the scene index.
Creating Agent Data Index (24 CPUs): 100%|â–ˆ| 150/150 [00:00<00:00, 15279.05it/s]
Structuring Agent Data Index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 62582.87it/s]
Restoring states from the checkpoint path at logs/2024-12-17 11:26:14/checkpoints/val_loss/iter6000_ep0_val_loss_val/loss.ckpt
/home/visier/anaconda3/envs/hf/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:345: The dirpath has changed from 'logs/2024-12-17 11:26:14/checkpoints/val_loss' to 'logs/2024-12-17 11:58:12/checkpoints/val_loss', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type    | Params
---------------------------------------
0 | dm         | DM      | 16.2 M
1 | ema_policy | DM      | 16.2 M
2 | vae        | LSTMVAE | 235 K
---------------------------------------
16.4 M    Trainable params
16.2 M    Non-trainable params
32.6 M    Total params
130.424   Total estimated model params size (MB)
Restored all states from the checkpoint at logs/2024-12-17 11:26:14/checkpoints/val_loss/iter6000_ep0_val_loss_val/loss.ckpt
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7447/7447 [05:59<00:00, 20.70it/s, v_num=5z4t]-----Epoch 0 has ended. Total Steps: 7448
/home/visier/anaconda3/envs/hf/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:154: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7447/7447 [29:03<00:00,  4.27it/s, v_num=5z4t]-----Epoch 1 has ended. Total Steps: 14895
Epoch 1, global step 8000: 'val/loss' reached 9.03444 (best 9.03444), saving model to 'logs/2024-12-17 11:58:12/checkpoints/val_loss/iter8000_ep1_val_loss_val/loss.ckpt' as top 1
Epoch 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 5544/7447 [21:40<07:26,  4.26it/s, v_num=5z4t]
Epoch 1, global step 10000: 'val/loss' was not in top 1
Epoch 1, global step 12000: 'val/loss' was not in top 1
Epoch 1, global step 14000: 'val/loss' was not in top 1
                                                                                                
Epoch 2, global step 16000: 'val/loss' was not in top 1
Epoch 2, global step 18000: 'val/loss' was not in top 1
Epoch 2, global step 20000: 'val/loss' was not in top 1
/home/visier/anaconda3/envs/hf/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...
